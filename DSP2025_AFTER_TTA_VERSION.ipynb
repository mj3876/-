{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mj3876/-/blob/main/DSP2025_AFTER_TTA_VERSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buRnZKeuvKjy"
      },
      "source": [
        "# [ë””ì§€íƒˆ ì‹ í˜¸ ì²˜ë¦¬] (2025)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ : ë”¥ëŸ¬ë‹ ê¸°ë°˜ í™”ì ì‹ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í–¥ìƒ\n",
        "\n",
        "---\n",
        "*   ë‹¤ì–‘í•œ ë°©ë²•ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì´ë©´ ë©ë‹ˆë‹¤. (DSP ê¸°ë²• í™œìš©, ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•, ëª¨ë¸ êµ¬ì¡° ê°œì„ , í•™ìŠµ ê¸°ë²• ê°œì„ , ì†ì‹¤ í•¨ìˆ˜ ê°œì„ )\n",
        "* ë°ì´í„°ì…‹ : 1ì´ˆ ë‹¨ìœ„ì˜ ìŒì„±ì´ ì•½ 4,648ê°œë¡œ, Train : Validation : Test = 8 : 1 : 1ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ë“¤ì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ (Accuracy)ì„ ë³´ì´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
        "* í™˜ê²½ êµ¬ì„± : ëŸ°ì–´ìŠ¤ í˜¹ì€ ê¹ƒí—ˆë¸Œ (https://github.com/JaeBinCHA7/DNN-based-Speaker-Identification-Tutorial) ì—ì„œ ì½”ë“œì™€ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ì…‹ê³¼ ì½”ë“œëŠ” êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì˜¬ë ¤ì£¼ì„¸ìš”.\n",
        "* GPUê°€ íƒ‘ì¬ëœ ë¡œì»¬ PCê°€ ì—†ëŠ” ë¶„ë“¤ì€.ipynb ë¥¼ í™œìš©í•´ì£¼ì‹œê³ , ê°€ìš©í•  ìˆ˜ ìˆëŠ” GPUê°€ ìˆëŠ” ë¶„ë“¤ì€ ê¹ƒí—ˆë¸Œ ì½”ë“œë¥¼ ì°¸ê³ í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "* ì œì¶œ : ì—¬ëŸ¬ë¶„ë“¤ì´ í•™ìŠµ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì¶”ë¡ í•œ í›„ ì–»ì€ \"test_predictions.csv\"ë¥¼ ëŸ°ì–´ìŠ¤ ìë£Œì‹¤ì— ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Nu4pwjGLIi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNTpsolU4Iq5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pi0AAtH6B62"
      },
      "outputs": [],
      "source": [
        "# Change Directory\n",
        "%cd '/content/drive/My Drive/dsp/project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyWGYoihmWjr"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/dsp_dataset/dsp_dataset_2025 (3).zip'\n",
        "extract_path = '/content/drive/MyDrive/dsp_dataset'\n",
        "dataset_dir = os.path.join(extract_path, 'dataset')\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"'{zip_path}' extracted to '{extract_path}'\")\n",
        "else:\n",
        "    print(f\"'{dataset_dir}' already exists. Skipping extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PbpzSbq8PWo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scgWhdfeG85h"
      },
      "outputs": [],
      "source": [
        "pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1gBPtu5G9D2"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Cell 2: ë¬µìŒ ì œê±° í•¨ìˆ˜ ì •ì˜\n",
        "import os\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def remove_silence(\n",
        "    audio_path: str,\n",
        "    output_path: str,\n",
        "    min_silence_len: int = 300,\n",
        "    silence_thresh: int = -40,\n",
        "    keep_silence: int = 100,\n",
        "    min_audio_len: int = 500,\n",
        "):\n",
        "    \"\"\"\n",
        "    ì˜¤ë””ì˜¤ì—ì„œ ë¬µìŒ ì œê±°\n",
        "\n",
        "    Parameters:\n",
        "        min_silence_len: ë¬µìŒìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ê¸¸ì´ (ms)\n",
        "        silence_thresh: ë¬µìŒ threshold (dBFS, ë‚®ì„ìˆ˜ë¡ ì—„ê²©)\n",
        "        keep_silence: ì•ë’¤ë¡œ ë‚¨ê¸¸ ë¬µìŒ (ms)\n",
        "        min_audio_len: ìµœì†Œ ì˜¤ë””ì˜¤ ê¸¸ì´ (ms)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "        # ë¬µìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \n",
        "        chunks = split_on_silence(\n",
        "            audio,\n",
        "            min_silence_len=min_silence_len,\n",
        "            silence_thresh=silence_thresh,\n",
        "            keep_silence=keep_silence,\n",
        "        )\n",
        "\n",
        "        # ì²­í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€\n",
        "        if len(chunks) == 0:\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "            return len(audio), len(audio), 0\n",
        "\n",
        "        # ëª¨ë“  ì²­í¬ ì´ì–´ë¶™ì´ê¸°\n",
        "        combined = AudioSegment.empty()\n",
        "        for chunk in chunks:\n",
        "            combined += chunk\n",
        "\n",
        "        # ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ìœ ì§€\n",
        "        if len(combined) < min_audio_len:\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "            return len(audio), len(audio), 0\n",
        "\n",
        "        # ì €ì¥\n",
        "        combined.export(output_path, format=\"wav\")\n",
        "\n",
        "        return len(audio), len(combined), len(audio) - len(combined)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Error: {e}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "def process_dataset(\n",
        "    csv_path: str,\n",
        "    input_root: str,\n",
        "    output_root: str,\n",
        "    min_silence_len: int = 300,\n",
        "    silence_thresh: int = -40,\n",
        "):\n",
        "    \"\"\"CSV ê¸°ë°˜ ë°ì´í„°ì…‹ ì „ì²´ ì²˜ë¦¬\"\"\"\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"ğŸ“ Processing {len(df)} files from {csv_path}\")\n",
        "\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "    total_orig = 0\n",
        "    total_new = 0\n",
        "    success = 0\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Removing silence\"):\n",
        "        rel_path = row['path']\n",
        "        input_path = os.path.join(input_root, rel_path)\n",
        "        output_path = os.path.join(output_root, rel_path)\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        orig_len, new_len, removed = remove_silence(\n",
        "            input_path, output_path,\n",
        "            min_silence_len=min_silence_len,\n",
        "            silence_thresh=silence_thresh,\n",
        "        )\n",
        "\n",
        "        if new_len > 0:\n",
        "            success += 1\n",
        "            total_orig += orig_len\n",
        "            total_new += new_len\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_all_datasets(\n",
        "    dataset_root: str = \"./dataset\",\n",
        "    output_root: str = \"./dataset_no_silence\",\n",
        "    min_silence_len: int = 300,\n",
        "    silence_thresh: int = -40,\n",
        "):\n",
        "    \"\"\"train, valid, test ëª¨ë‘ ì²˜ë¦¬\"\"\"\n",
        "\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        csv_path = os.path.join(dataset_root, f\"{split}.csv\")\n",
        "\n",
        "        if not os.path.exists(csv_path):\n",
        "            print(f\"âš ï¸  Skipping {split}.csv (not found)\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'#'*60}\")\n",
        "        print(f\"Processing {split.upper()} dataset\")\n",
        "        print(f\"{'#'*60}\")\n",
        "\n",
        "        process_dataset(\n",
        "            csv_path=csv_path,\n",
        "            input_root=dataset_root,\n",
        "            output_root=output_root,\n",
        "            min_silence_len=min_silence_len,\n",
        "            silence_thresh=silence_thresh,\n",
        "        )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"All datasets processed!\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cell 5: ì „ì²´ ë°ì´í„°ì…‹ ë¬µìŒ ì œê±° ì‹¤í–‰\n",
        "# ì£¼ì˜: ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
        "process_all_datasets(\n",
        "    dataset_root=\"/content/drive/MyDrive/dsp_dataset/dataset\",\n",
        "    output_root=\"/content/drive/MyDrive/dsp_dataset/dataset/dataset_no_silence\",\n",
        "    min_silence_len=300,  # í•„ìš”ì‹œ ì¡°ì •\n",
        "    silence_thresh=-40    # í•„ìš”ì‹œ ì¡°ì •\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lohgggFctQIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvrZvH-48uLT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### í›ˆë ¨ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh1qL8YxG9L6"
      },
      "outputs": [],
      "source": [
        "def build_args():\n",
        "    p = argparse.ArgumentParser(description=\"í™”ì ì‹ë³„ ëª¨ë¸ í•™ìŠµ\")\n",
        "\n",
        "    # ì‹¤í—˜ ì„¤ì •\n",
        "    p.add_argument(\"--exp_name\", type=str, default=\"EXP_LS_TTA\",\n",
        "                   help=\"ì‹¤í—˜ ì´ë¦„\")\n",
        "\n",
        "    # ë°ì´í„° ê²½ë¡œ\n",
        "    dataset_root_default = \"/content/drive/MyDrive/dsp_dataset/dataset\"\n",
        "    p.add_argument(\"--dataset_root\", type=str, default=dataset_root_default, help=\"ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ\")\n",
        "    p.add_argument(\"--train_csv\", type=str, default=os.path.join(dataset_root_default, \"train.csv\"))\n",
        "    p.add_argument(\"--valid_csv\", type=str, default=os.path.join(dataset_root_default, \"valid.csv\"))\n",
        "    p.add_argument(\"--test_csv\", type=str, default=os.path.join(dataset_root_default, \"test.csv\"))\n",
        "\n",
        "\n",
        "\n",
        "    # ì˜¤ë””ì˜¤ / Feature\n",
        "    p.add_argument(\"--sr\", type=int, default=16000)\n",
        "    p.add_argument(\"--duration\", type=float, default=1.0, help=\"ìƒ˜í”Œ ê¸¸ì´ (ì´ˆ)\")\n",
        "    p.add_argument(\"--n_mels\", type=int, default=80)\n",
        "    p.add_argument(\"--n_fft\", type=int, default=400)\n",
        "    p.add_argument(\"--hop_length\", type=int, default=160)\n",
        "\n",
        "    # í•™ìŠµ ì„¤ì •\n",
        "    p.add_argument(\"--epochs\", type=int, default=40)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--min_lr\", type=float, default=1e-6)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--grad_clip\", type=float, default=5.0)\n",
        "    p.add_argument(\"--num_workers\", type=int, default=0)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--cpu\", action=\"store_true\")\n",
        "\n",
        "    # ëª¨ë¸ êµ¬ì¡°\n",
        "    p.add_argument(\"--encoder_dim\", type=int, default=256)\n",
        "    p.add_argument(\"--num_blocks\", type=int, default=6)\n",
        "    p.add_argument(\"--num_heads\", type=int, default=4)\n",
        "    p.add_argument(\"--ff_expansion\", type=int, default=4)\n",
        "    p.add_argument(\"--conv_kernel_size\", type=int, default=15)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.1)\n",
        "\n",
        "    # Label Smoothing\n",
        "    p.add_argument(\"--label_smoothing\", type=float, default=0.1,\n",
        "                   help=\"Label smoothing ê³„ìˆ˜\")\n",
        "\n",
        "    # ì¶œë ¥ ê²½ë¡œ\n",
        "    p.add_argument(\"--out_dir\", type=str, default=\"./logs\", help=\"ë¡œê·¸ ì €ì¥ ê²½ë¡œ\")\n",
        "\n",
        "    return p.parse_known_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjZPK3Ls82Er"
      },
      "source": [
        "---\n",
        "\n",
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLWVggP6HBmh"
      },
      "outputs": [],
      "source": [
        "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_csv(csv_path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Test íŒŒì¼ì€ speaker_id ì—†ì„ ìˆ˜ ìˆìŒ\n",
        "    if \"test\" not in csv_path.lower():\n",
        "        required = {\"path\", \"speaker_id\"}\n",
        "        if not required.issubset(df.columns):\n",
        "            raise ValueError(f\"{csv_path} must contain columns: {required}\")\n",
        "    else:\n",
        "        if \"path\" not in df.columns:\n",
        "            raise ValueError(f\"{csv_path} must contain 'path' column\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_label_map(train_df: pd.DataFrame, valid_df: pd.DataFrame) -> Dict[str, int]:\n",
        "    spk_all = sorted(set(train_df[\"speaker_id\"].astype(str)) | set(valid_df[\"speaker_id\"].astype(str)))\n",
        "    label_map = {spk: i for i, spk in enumerate(spk_all)}\n",
        "    return label_map\n",
        "\n",
        "\n",
        "def get_lr(optimizer: torch.optim.Optimizer) -> float:\n",
        "    for pg in optimizer.param_groups:\n",
        "        return float(pg.get(\"lr\", 0.0))\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def fmt_pct(x: float) -> str:\n",
        "    return f\"{100.0 * x:6.2f}%\"\n",
        "\n",
        "\n",
        "def print_epoch_header(epoch: int, total_epochs: int):\n",
        "    bar = \"=\" * 66\n",
        "    print(f\"\\n{bar}\\n[Epoch {epoch:03d}/{total_epochs:03d}] START\\n{bar}\")\n",
        "\n",
        "\n",
        "def print_epoch_summary(epoch: int, train_loss: float, train_acc: float,\n",
        "                        val_loss: float, val_acc: float, best_acc: float,\n",
        "                        lr: float, elapsed: float, ckpt_best: bool, csv_path: str):\n",
        "    # Epoch ê²°ê³¼ ì¶œë ¥\n",
        "    bar = \"-\" * 66\n",
        "    print(f\"{bar}\")\n",
        "    print(f\"| Split |   Loss   |   Acc    |\")\n",
        "    print(f\"{bar}\")\n",
        "    print(f\"| Train | {train_loss:8.4f} | {fmt_pct(train_acc)} |\")\n",
        "    print(f\"| Valid | {val_loss:8.4f} | {fmt_pct(val_acc)} |\")\n",
        "    print(f\"{bar}\")\n",
        "    print(f\"| Best Acc: {fmt_pct(best_acc)} | LR: {lr:.3e} | Time: {elapsed:.1f}s |\")\n",
        "    print(f\"{bar}\")\n",
        "    if ckpt_best:\n",
        "        print(\"âœ“ New BEST model saved.\")\n",
        "    print(\"=\" * 66)\n",
        "\n",
        "    # CSV ë¡œê¹…\n",
        "    row = dict(\n",
        "        epoch=epoch,\n",
        "        train_loss=float(train_loss),\n",
        "        train_acc=float(train_acc),\n",
        "        valid_loss=float(val_loss),\n",
        "        valid_acc=float(val_acc),\n",
        "        best_acc=float(best_acc),\n",
        "        lr=float(lr),\n",
        "        time_sec=float(elapsed),\n",
        "    )\n",
        "    header = not os.path.exists(csv_path)\n",
        "    df = pd.DataFrame([row])\n",
        "    ensure_dir(os.path.dirname(csv_path))\n",
        "    df.to_csv(csv_path, mode=\"a\", index=False, header=header)\n",
        "\n",
        "\n",
        "# ì˜¤ë””ì˜¤ ì²˜ë¦¬\n",
        "def load_wav(full_path: str, target_sr: int) -> np.ndarray:\n",
        "    try:\n",
        "        y, sr = sf.read(full_path, always_2d=False)\n",
        "        if y.ndim > 1:\n",
        "            y = np.mean(y, axis=1)  # mixdown to mono\n",
        "        if sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_best\")\n",
        "        return y.astype(np.float32)\n",
        "    except Exception:\n",
        "        y, _ = librosa.load(full_path, sr=target_sr, mono=True)\n",
        "        return y.astype(np.float32)\n",
        "\n",
        "\n",
        "def fix_length(y: np.ndarray, num_samples: int) -> np.ndarray:\n",
        "    if len(y) == num_samples:\n",
        "        return y\n",
        "    if len(y) > num_samples:\n",
        "        return y[:num_samples]\n",
        "    pad = num_samples - len(y)\n",
        "    return np.pad(y, (0, pad), mode=\"constant\")\n",
        "\n",
        "\n",
        "def wav_to_logmel(\n",
        "        y: np.ndarray,\n",
        "        sr: int,\n",
        "        n_mels: int = 80,\n",
        "        n_fft: int = 400,  # 25 ms @ 16k\n",
        "        hop_length: int = 160,  # 10 ms @ 16k\n",
        "        fmin: int = 20,\n",
        "        fmax: int = 7600,\n",
        ") -> np.ndarray:\n",
        "    S = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_fft=n_fft, hop_length=hop_length,\n",
        "        n_mels=n_mels, fmin=fmin, fmax=fmax, power=2.0\n",
        "    )\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "    return S_db.astype(np.float32)  # [n_mels, T]\n",
        "\n",
        "\n",
        "# Cepstral mean and variance normalization\n",
        "def cmvn(x: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n",
        "    mu = x.mean(axis=1, keepdims=True)\n",
        "    std = x.std(axis=1, keepdims=True)\n",
        "    return (x - mu) / (std + eps)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# [Label Smoothing Cross Entropy]\n",
        "# ============================================\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Label Smoothing Cross Entropy Loss\n",
        "\n",
        "    ì¼ë°˜ Cross EntropyëŠ” ì •ë‹µ í´ë˜ìŠ¤ì— 100% í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ”ë°,\n",
        "    Label Smoothingì€ ì •ë‹µì— (1-Îµ)ì˜ í™•ë¥ ì„, ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ì— Îµ/(K-1)ì˜ í™•ë¥ ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "    íš¨ê³¼:\n",
        "    - Overfitting ë°©ì§€\n",
        "    - ëª¨ë¸ì´ ê³¼ë„í•˜ê²Œ í™•ì‹ í•˜ëŠ” ê²ƒì„ ë°©ì§€ (calibration ê°œì„ )\n",
        "    - ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "\n",
        "    Args:\n",
        "        smoothing: smoothing ì •ë„ (0.0 ~ 1.0)\n",
        "                   0.0ì´ë©´ ì¼ë°˜ CrossEntropyì™€ ë™ì¼\n",
        "                   ë³´í†µ 0.1 ì •ë„ ì‚¬ìš©\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (pred.size(-1) - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV2obcO4865u"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ë°ì´í„°ë¡œë”\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOweddWLkZ_A"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import torchaudio\n",
        "\n",
        "def load_audio_safe(path: str, target_sr: int = 16000):\n",
        "    \"\"\"soundfileì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë¡œë“œ\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "\n",
        "        waveform, sr = sf.read(path, dtype='float32')\n",
        "\n",
        "        # monoë¡œ ë³€í™˜\n",
        "        if len(waveform.shape) > 1:\n",
        "            waveform = waveform.mean(axis=1)\n",
        "\n",
        "        waveform = torch.from_numpy(waveform).unsqueeze(0)  # [1, T]\n",
        "\n",
        "        # ë¦¬ìƒ˜í”Œë§\n",
        "        if sr != target_sr:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "        return waveform, target_sr\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to load {path}: {e}\")\n",
        "        silent_waveform = torch.zeros(1, target_sr)\n",
        "        return silent_waveform, target_sr\n",
        "\n",
        "\n",
        "# For training\n",
        "class SpeakerDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            csv_path: str,\n",
        "            dataset_root: str = \"./dataset\",\n",
        "            label_map: Dict[str, int] = None,\n",
        "            target_sr: int = 16000,\n",
        "            duration_sec: float = 1.0,\n",
        "            n_mels: int = 80,\n",
        "            n_fft: int = 400,\n",
        "            hop_length: int = 160,\n",
        "            apply_cmvn: bool = True,\n",
        "    ):\n",
        "        self.df = load_csv(csv_path).copy()\n",
        "        self.df[\"abs_path\"] = self.df[\"path\"].apply(lambda p: os.path.join(dataset_root, p))\n",
        "        self.label_map = label_map if label_map else build_label_map(self.df)\n",
        "        self.df[\"label\"] = self.df[\"speaker_id\"].astype(str).map(self.label_map)\n",
        "\n",
        "        before = len(self.df)\n",
        "        self.df = self.df.dropna(subset=[\"label\"])\n",
        "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
        "        after = len(self.df)\n",
        "        if after < before:\n",
        "            print(f\"[WARN] Dropped {before - after} rows due to unmapped speaker_id.\")\n",
        "\n",
        "        self.target_sr = target_sr\n",
        "        self.target_len = int(duration_sec * target_sr)\n",
        "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=target_sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
        "        )\n",
        "        self.apply_cmvn = apply_cmvn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        waveform, sr = load_audio_safe(row[\"abs_path\"], self.target_sr)\n",
        "\n",
        "        wav_len = waveform.size(1)\n",
        "        if wav_len < self.target_len:\n",
        "            waveform = F.pad(waveform, (0, self.target_len - wav_len))\n",
        "        elif wav_len > self.target_len:\n",
        "            # Random crop (í•µì‹¬!)\n",
        "            start = random.randint(0, wav_len - self.target_len)\n",
        "            waveform = waveform[:, start:start + self.target_len]\n",
        "\n",
        "        mel = self.mel_spec(waveform)\n",
        "        mel = torch.log(mel + 1e-9)\n",
        "        if self.apply_cmvn:\n",
        "            mel = (mel - mel.mean()) / (mel.std() + 1e-9)\n",
        "\n",
        "        return mel, row[\"label\"]\n",
        "\n",
        "\n",
        "# For test\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path: str,\n",
        "        dataset_root: str = \"./dataset\",\n",
        "        target_sr: int = 16000,\n",
        "        duration_sec: float = 1.0,\n",
        "        n_mels: int = 80,\n",
        "        n_fft: int = 400,\n",
        "        hop_length: int = 160,\n",
        "        apply_cmvn: bool = True,\n",
        "    ):\n",
        "        self.df = load_csv(csv_path).copy()\n",
        "        self.df[\"abs_path\"] = self.df[\"path\"].apply(lambda p: os.path.join(dataset_root, p))\n",
        "        self.target_sr = target_sr\n",
        "        self.target_len = int(duration_sec * target_sr)\n",
        "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=target_sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
        "        )\n",
        "        self.apply_cmvn = apply_cmvn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        waveform, sr = load_audio_safe(row[\"abs_path\"], self.target_sr)\n",
        "\n",
        "        wav_len = waveform.size(1)\n",
        "        if wav_len < self.target_len:\n",
        "            waveform = F.pad(waveform, (0, self.target_len - wav_len))\n",
        "        else:\n",
        "            waveform = waveform[:, :self.target_len]\n",
        "\n",
        "        mel = self.mel_spec(waveform)\n",
        "        mel = torch.log(mel + 1e-9)\n",
        "        if self.apply_cmvn:\n",
        "            mel = (mel - mel.mean()) / (mel.std() + 1e-9)\n",
        "\n",
        "        return mel, os.path.basename(row[\"path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-aoreB5kJLw"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ëª¨ë¸ ì„ ì–¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbkIYctIHF8m"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# MFA-Conformer Components\n",
        "# ============================================\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    \"\"\"Conformer Block\"\"\"\n",
        "    def __init__(self, dim: int, num_heads: int = 4, ff_expansion: int = 4,\n",
        "                 conv_kernel_size: int = 15, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # First Feed Forward (Macaron-style)\n",
        "        self.ffn1 = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, ff_expansion * dim),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_expansion * dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.ln_attn = nn.LayerNorm(dim)\n",
        "        self.mha = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.dropout_attn = nn.Dropout(dropout)\n",
        "\n",
        "        # Convolution Module\n",
        "        self.ln_conv = nn.LayerNorm(dim)\n",
        "        self.pointwise_conv1 = nn.Conv1d(dim, 2 * dim, kernel_size=1)\n",
        "        self.depthwise_conv = nn.Conv1d(dim, dim, kernel_size=conv_kernel_size,\n",
        "                                       padding=(conv_kernel_size - 1) // 2, groups=dim)\n",
        "        self.bn = nn.BatchNorm1d(dim)\n",
        "        self.swish = Swish()\n",
        "        self.pointwise_conv2 = nn.Conv1d(dim, dim, kernel_size=1)\n",
        "        self.dropout_conv = nn.Dropout(dropout)\n",
        "\n",
        "        # Second Feed Forward\n",
        "        self.ffn2 = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, ff_expansion * dim),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_expansion * dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.norm_out = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        # Feed Forward 1 (half step)\n",
        "        x = x + 0.5 * self.ffn1(x)\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        x_normed = self.ln_attn(x)\n",
        "        attn_out, _ = self.mha(x_normed, x_normed, x_normed)\n",
        "        x = x + self.dropout_attn(attn_out)\n",
        "\n",
        "        # Convolution Module\n",
        "        residual = x\n",
        "        x = self.ln_conv(x)\n",
        "        x = x.transpose(1, 2)  # [B, D, T]\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = F.glu(x, dim=1)  # [B, D, T]\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.swish(x)\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout_conv(x)\n",
        "        x = x.transpose(1, 2)  # [B, T, D]\n",
        "        x = residual + x\n",
        "\n",
        "        # Feed Forward 2 (half step)\n",
        "        x = x + 0.5 * self.ffn2(x)\n",
        "\n",
        "        return self.norm_out(x)\n",
        "\n",
        "\n",
        "class AttentiveStatsPooling(nn.Module):\n",
        "    \"\"\"Attentive Statistics Pooling\"\"\"\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        attn_scores = self.attention(x)  # [B, T, 1]\n",
        "        attn_weights = F.softmax(attn_scores, dim=1)  # [B, T, 1]\n",
        "\n",
        "        # Weighted mean\n",
        "        weighted_mean = torch.sum(x * attn_weights, dim=1)  # [B, D]\n",
        "\n",
        "        # Weighted std\n",
        "        mean = torch.mean(x, dim=1, keepdim=True)  # [B, 1, D]\n",
        "        variance = torch.sum(attn_weights * (x - mean) ** 2, dim=1)  # [B, D]\n",
        "        weighted_std = torch.sqrt(variance + 1e-9)\n",
        "\n",
        "        # Concatenate mean and std\n",
        "        pooled = torch.cat([weighted_mean, weighted_std], dim=-1)  # [B, 2*D]\n",
        "        return pooled\n",
        "\n",
        "\n",
        "class MFAConformerSpeakerNet(nn.Module):\n",
        "    \"\"\"Multi-scale Feature Aggregation Conformer\"\"\"\n",
        "    def __init__(self, n_classes: int, input_dim: int = 80, encoder_dim: int = 256,\n",
        "                 num_blocks: int = 6, num_heads: int = 4, ff_expansion: int = 4,\n",
        "                 conv_kernel_size: int = 15, dropout: float = 0.1, embedding_dim: int = 192):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution Subsampling\n",
        "        self.conv_subsample = nn.Sequential(\n",
        "            nn.Conv2d(1, encoder_dim, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(encoder_dim, encoder_dim, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Linear projection\n",
        "        self.input_proj = nn.Linear((input_dim // 4) * encoder_dim, encoder_dim)\n",
        "\n",
        "        # Conformer Blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ConformerBlock(encoder_dim, num_heads, ff_expansion, conv_kernel_size, dropout)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale Feature Aggregation\n",
        "        self.mfa_norm = nn.LayerNorm(encoder_dim * num_blocks)\n",
        "\n",
        "        # Attentive Statistics Pooling\n",
        "        self.asp = AttentiveStatsPooling(encoder_dim * num_blocks)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(encoder_dim * num_blocks * 2, embedding_dim),\n",
        "            nn.BatchNorm1d(embedding_dim)\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(embedding_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 1, F, T]\n",
        "        x = self.conv_subsample(x)  # [B, C, F/4, T/4]\n",
        "\n",
        "        B, C, F, T = x.shape\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().view(B, T, C * F)  # [B, T, C*F]\n",
        "        x = self.input_proj(x)  # [B, T, D]\n",
        "\n",
        "        # Multi-scale Feature Aggregation\n",
        "        block_outputs = []\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            block_outputs.append(x)\n",
        "\n",
        "        # Concatenate all block outputs\n",
        "        x = torch.cat(block_outputs, dim=-1)  # [B, T, D*num_blocks]\n",
        "        x = self.mfa_norm(x)\n",
        "\n",
        "        # Attentive Statistics Pooling\n",
        "        x = self.asp(x)  # [B, 2*D*num_blocks]\n",
        "\n",
        "        # Embedding\n",
        "        emb = self.embedding(x)  # [B, embedding_dim]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(emb)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtHoqZT_8_pt"
      },
      "source": [
        "---\n",
        "\n",
        "### í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbSmPHxXHLk-"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# argsë¥¼ ìˆ˜ì •í•´ì„œ ìƒˆ ë°ì´í„°ì…‹ ê²½ë¡œ ì‚¬ìš©\n",
        "args = build_args()\n",
        "args[0].exp_name = \"EXP_NoSilence\"\n",
        "args[0].dataset_root = \"/content/dataset_no_silence/\"  # ë³€ê²½ëœ ê²½ë¡œ\n",
        "args[0].epochs = 50\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        set_seed(args.seed)\n",
        "\n",
        "        # ---- Resolve experiment directories ----\n",
        "        self.logs_root = os.path.abspath(args.out_dir)\n",
        "        self.exp_dir = os.path.join(self.logs_root, args.exp_name)\n",
        "        self.tb_dir = os.path.join(self.exp_dir, \"tensorboard\")\n",
        "        self.ckpt_dir = os.path.join(self.exp_dir, \"checkpoints\")\n",
        "        ensure_dir(self.tb_dir)\n",
        "        ensure_dir(self.ckpt_dir)\n",
        "\n",
        "        # Save run config snapshot for reproducibility\n",
        "        with open(os.path.join(self.exp_dir, \"run_config.json\"), \"w\") as f:\n",
        "            json.dump(vars(args), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # TensorBoard writer under experiment dir\n",
        "        self.writer = SummaryWriter(log_dir=self.tb_dir)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "            print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            print(\"[WARNING] CUDA not available. Using CPU instead.\")\n",
        "\n",
        "        # ---- Data & Label map ----\n",
        "        train_df = load_csv(args.train_csv)\n",
        "        valid_df = load_csv(args.valid_csv)\n",
        "        label_map = build_label_map(train_df, valid_df)\n",
        "        self.label_map = label_map\n",
        "        self.inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "        # save label map in experiment dir\n",
        "        with open(os.path.join(self.exp_dir, \"label_map.json\"), \"w\") as f:\n",
        "            json.dump(self.label_map, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        self.num_classes = len(self.label_map)\n",
        "        print(f\"[INFO] #speakers (classes) = {self.num_classes}\")\n",
        "        print(f\"[INFO] Device: {self.device}\")\n",
        "        print(f\"[INFO] Experiment dir: {self.exp_dir}\")\n",
        "\n",
        "        # ---- Datasets ----\n",
        "        self.train_set = SpeakerDataset(\n",
        "            csv_path=args.train_csv,\n",
        "            dataset_root=args.dataset_root,\n",
        "            label_map=self.label_map,\n",
        "            target_sr=args.sr,\n",
        "            duration_sec=args.duration,\n",
        "            n_mels=args.n_mels,\n",
        "            n_fft=args.n_fft,\n",
        "            hop_length=args.hop_length,\n",
        "            apply_cmvn=True,\n",
        "        )\n",
        "        self.valid_set = SpeakerDataset(\n",
        "            csv_path=args.valid_csv,\n",
        "            dataset_root=args.dataset_root,\n",
        "            label_map=self.label_map,\n",
        "            target_sr=args.sr,\n",
        "            duration_sec=args.duration,\n",
        "            n_mels=args.n_mels,\n",
        "            n_fft=args.n_fft,\n",
        "            hop_length=args.hop_length,\n",
        "            apply_cmvn=True,\n",
        "        )\n",
        "\n",
        "        # ---- Dataloaders ----\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_set, batch_size=args.batch_size, shuffle=True,\n",
        "            num_workers=args.num_workers, pin_memory=True, drop_last=True\n",
        "        )\n",
        "        self.valid_loader = DataLoader(\n",
        "            self.valid_set, batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.num_workers, pin_memory=True, drop_last=False\n",
        "        )\n",
        "\n",
        "        # ---- Model/Opt ----\n",
        "        self.model = MFAConformerSpeakerNet(\n",
        "            n_classes=self.num_classes,\n",
        "            input_dim=args.n_mels,\n",
        "            encoder_dim=args.encoder_dim,\n",
        "            num_blocks=args.num_blocks,\n",
        "            num_heads=args.num_heads,\n",
        "            ff_expansion=args.ff_expansion,\n",
        "            conv_kernel_size=args.conv_kernel_size,\n",
        "            dropout=args.dropout\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Print model parameter count\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        print(f\"[INFO] Total parameters: {total_params:,}\")\n",
        "\n",
        "        # ---- Loss function: Label Smoothing ----\n",
        "        if args.label_smoothing > 0:\n",
        "            self.criterion = LabelSmoothingCrossEntropy(smoothing=args.label_smoothing)\n",
        "            print(f\"[INFO] Using Label Smoothing with smoothing={args.label_smoothing}\")\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "            print(f\"[INFO] Using standard CrossEntropyLoss\")\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=4, gamma=0.5)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.best_acc = 0.0\n",
        "\n",
        "        # CSV metrics path (under experiment dir)\n",
        "        self.metrics_csv = os.path.join(self.exp_dir, \"metrics_log.csv\")\n",
        "\n",
        "        # tqdm common bar format\n",
        "        self.bar_format = (\n",
        "            \"{l_bar}{bar:30}| {n_fmt}/{total_fmt} \"\n",
        "            \"[{elapsed}<{remaining}, {rate_fmt}]\"\n",
        "        )\n",
        "\n",
        "    def train_one_epoch(self, epoch: int):\n",
        "        self.model.train()\n",
        "\n",
        "        total_seen = 0\n",
        "        total_correct = 0\n",
        "        loss_sum = 0.0\n",
        "\n",
        "        lr_now = get_lr(self.optimizer)\n",
        "        pbar = tqdm(\n",
        "            self.train_loader,\n",
        "            desc=f\"Train {epoch:03d} | lr={lr_now:.2e}\",\n",
        "            ncols=116,\n",
        "            bar_format=self.bar_format,\n",
        "            mininterval=0.5\n",
        "        )\n",
        "\n",
        "        tic = time.time()\n",
        "        for mel, label in pbar:\n",
        "            bs = label.size(0)\n",
        "            mel = mel.to(self.device)\n",
        "            label = label.to(self.device)\n",
        "\n",
        "            logits = self.model(mel)\n",
        "            loss = self.criterion(logits, label)\n",
        "\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.grad_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct = (pred == label).sum().item()\n",
        "\n",
        "                total_seen += bs\n",
        "                total_correct += correct\n",
        "                loss_sum += loss.item() * bs\n",
        "\n",
        "                elapsed = max(1e-9, time.time() - tic)\n",
        "                ips = total_seen / elapsed\n",
        "                cur_loss = loss_sum / max(1, total_seen)\n",
        "                cur_acc = total_correct / max(1, total_seen)\n",
        "\n",
        "            pbar.set_postfix(\n",
        "                loss=f\"{cur_loss:7.4f}\",\n",
        "                acc=f\"{cur_acc:6.4f}\",\n",
        "                ips=f\"{ips:6.1f}/s\",\n",
        "                lr=f\"{get_lr(self.optimizer):.2e}\",\n",
        "            )\n",
        "            self.global_step += 1\n",
        "\n",
        "        epoch_loss = loss_sum / max(1, total_seen)\n",
        "        epoch_acc = total_correct / max(1, total_seen)\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, epoch: int):\n",
        "        self.model.eval()\n",
        "\n",
        "        total_seen = 0\n",
        "        total_correct = 0\n",
        "        loss_sum = 0.0\n",
        "\n",
        "        pbar = tqdm(\n",
        "            self.valid_loader,\n",
        "            desc=f\"Valid {epoch:03d}\",\n",
        "            ncols=116,\n",
        "            bar_format=self.bar_format,\n",
        "            mininterval=0.5\n",
        "        )\n",
        "        tic = time.time()\n",
        "        for mel, label in pbar:\n",
        "            bs = label.size(0)\n",
        "            mel = mel.to(self.device)\n",
        "            label = label.to(self.device)\n",
        "\n",
        "            logits = self.model(mel)\n",
        "            loss = self.criterion(logits, label)\n",
        "\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct = (pred == label).sum().item()\n",
        "\n",
        "            total_seen += bs\n",
        "            total_correct += correct\n",
        "            loss_sum += loss.item() * bs\n",
        "\n",
        "            elapsed = max(1e-9, time.time() - tic)\n",
        "            ips = total_seen / elapsed\n",
        "            cur_loss = loss_sum / max(1, total_seen)\n",
        "            cur_acc = total_correct / max(1, total_seen)\n",
        "\n",
        "            pbar.set_postfix(\n",
        "                val_loss=f\"{cur_loss:7.4f}\",\n",
        "                val_acc=f\"{cur_acc:6.4f}\",\n",
        "                ips=f\"{ips:6.1f}/s\",\n",
        "            )\n",
        "\n",
        "        epoch_loss = loss_sum / max(1, total_seen)\n",
        "        epoch_acc = total_correct / max(1, total_seen)\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def save_checkpoint(self, epoch: int):\n",
        "        \"\"\"Save only best.pth (when improved).\"\"\"\n",
        "        state = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": self.model.state_dict(),\n",
        "            \"optimizer_state\": self.optimizer.state_dict(),\n",
        "            \"scheduler_state\": self.scheduler.state_dict(),\n",
        "            \"best_acc\": self.best_acc,\n",
        "            \"label_map\": self.label_map,\n",
        "            \"args\": vars(self.args),\n",
        "        }\n",
        "        best_path = os.path.join(self.ckpt_dir, \"best.pth\")\n",
        "        torch.save(state, best_path)\n",
        "        print(f\"[Checkpoint] BEST updated â†’ {best_path}\")\n",
        "\n",
        "    def run(self):\n",
        "        total_epochs = self.args.epochs\n",
        "        for epoch in range(1, total_epochs + 1):\n",
        "            print_epoch_header(epoch, total_epochs)\n",
        "            t0 = time.time()\n",
        "\n",
        "            train_loss, train_acc = self.train_one_epoch(epoch)\n",
        "            val_loss, val_acc = self.evaluate(epoch)\n",
        "\n",
        "            # TensorBoard\n",
        "            self.writer.add_scalar(\"train/loss\", train_loss, epoch)\n",
        "            self.writer.add_scalar(\"train/acc\", train_acc, epoch)\n",
        "            self.writer.add_scalar(\"valid/loss\", val_loss, epoch)\n",
        "            self.writer.add_scalar(\"valid/acc\", val_acc, epoch)\n",
        "            self.writer.add_scalar(\"train/lr\", get_lr(self.optimizer), epoch)\n",
        "\n",
        "            # Scheduler step AFTER logging current LR\n",
        "            self.scheduler.step()\n",
        "            cur_lr = get_lr(self.optimizer)\n",
        "\n",
        "            # Best & CKPT\n",
        "            is_best = val_acc > self.best_acc\n",
        "            if is_best:\n",
        "                self.best_acc = val_acc\n",
        "                self.save_checkpoint(epoch)\n",
        "\n",
        "            elapsed = time.time() - t0\n",
        "            print_epoch_summary(\n",
        "                epoch=epoch,\n",
        "                train_loss=train_loss, train_acc=train_acc,\n",
        "                val_loss=val_loss, val_acc=val_acc,\n",
        "                best_acc=self.best_acc, lr=cur_lr,\n",
        "                elapsed=elapsed, ckpt_best=is_best,\n",
        "                csv_path=self.metrics_csv\n",
        "            )\n",
        "\n",
        "        print(\"[INFO] Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTXh5LEzmGiK"
      },
      "source": [
        "---\n",
        "\n",
        "### í…ŒìŠ¤í„°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9_BX8sMmJmo"
      },
      "outputs": [],
      "source": [
        "def load_label_map(exp_dir: str) -> Dict[str, int]:\n",
        "    lm_path = os.path.join(exp_dir, \"label_map.json\")\n",
        "    if not os.path.exists(lm_path):\n",
        "        raise FileNotFoundError(f\"label_map.json not found at {lm_path}\")\n",
        "    with open(lm_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def load_checkpoint(exp_dir: str, device: torch.device):\n",
        "    ckpt_path = os.path.join(exp_dir, \"checkpoints\", \"best.pth\")\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"best.pth not found at {ckpt_path}\")\n",
        "    state = torch.load(ckpt_path, map_location=device)\n",
        "    return state\n",
        "\n",
        "\n",
        "def infer(args, use_tta=False, tta_crops=5):\n",
        "    \"\"\"\n",
        "    Test ë°ì´í„° ì¶”ë¡ \n",
        "\n",
        "    Args:\n",
        "        args: í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "        use_tta: TTA ì‚¬ìš© ì—¬ë¶€\n",
        "        tta_crops: TTA crop íšŸìˆ˜\n",
        "    \"\"\"\n",
        "    # ê²½ë¡œ ì„¤ì •\n",
        "    logs_root = os.path.abspath(args.out_dir)\n",
        "    exp_dir = os.path.join(logs_root, args.exp_name)\n",
        "    ensure_dir(exp_dir)\n",
        "\n",
        "    # Label map ë¡œë“œ\n",
        "    label_map = load_label_map(exp_dir)\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "    num_classes = len(inv_label_map)\n",
        "\n",
        "    # Device ì„¤ì •\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
        "    print(f\"[INFO] Device: {device}\")\n",
        "    print(f\"[INFO] Experiment dir: {exp_dir}\")\n",
        "    if use_tta:\n",
        "        print(f\"[INFO] Using TTA with {tta_crops} crops\")\n",
        "\n",
        "    # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "    dataset = TestDataset(\n",
        "        csv_path=args.test_csv,\n",
        "        dataset_root=args.dataset_root,\n",
        "        target_sr=args.sr,\n",
        "        duration_sec=args.duration,\n",
        "        n_mels=args.n_mels,\n",
        "        n_fft=args.n_fft,\n",
        "        hop_length=args.hop_length,\n",
        "        apply_cmvn=True,\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        dataset, batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=args.num_workers, pin_memory=True, drop_last=False,\n",
        "    )\n",
        "\n",
        "    # ëª¨ë¸ ë¡œë“œ\n",
        "    model = MFAConformerSpeakerNet(\n",
        "        n_classes=num_classes,\n",
        "        input_dim=args.n_mels,\n",
        "        encoder_dim=args.encoder_dim,\n",
        "        num_blocks=args.num_blocks,\n",
        "        num_heads=args.num_heads,\n",
        "        ff_expansion=args.ff_expansion,\n",
        "        conv_kernel_size=args.conv_kernel_size,\n",
        "        dropout=args.dropout\n",
        "    ).to(device)\n",
        "    state = load_checkpoint(exp_dir, device)\n",
        "    model.load_state_dict(state[\"model_state\"], strict=True)\n",
        "    model.eval()\n",
        "\n",
        "    # ì¶”ë¡ \n",
        "    filenames = []\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"Inference\", ncols=116)\n",
        "        for mel, filename in pbar:\n",
        "            mel = mel.to(device)\n",
        "\n",
        "            if use_tta:\n",
        "                # TTA: ì—¬ëŸ¬ ë²ˆ cropí•´ì„œ í‰ê· \n",
        "                B, C, F, T = mel.shape\n",
        "                target_len = int(args.duration * args.sr / args.hop_length) + 1\n",
        "\n",
        "                logits_list = []\n",
        "                for _ in range(tta_crops):\n",
        "                    if T > target_len:\n",
        "                        start = random.randint(0, T - target_len)\n",
        "                        mel_crop = mel[:, :, :, start:start + target_len]\n",
        "                    else:\n",
        "                        mel_crop = mel\n",
        "\n",
        "                    logits = model(mel_crop)\n",
        "                    logits_list.append(logits)\n",
        "\n",
        "                logits = torch.stack(logits_list).mean(dim=0)\n",
        "            else:\n",
        "                logits = model(mel)\n",
        "\n",
        "            pred_idx = logits.argmax(dim=1).cpu().numpy().tolist()\n",
        "            pred_spk = [inv_label_map[int(i)] for i in pred_idx]\n",
        "\n",
        "            filenames.extend(filename)\n",
        "            preds.extend(pred_spk)\n",
        "\n",
        "    # CSV ì €ì¥ (logs/predictions/ í´ë”ì—)\n",
        "    predictions_dir = os.path.join(logs_root, \"predictions\")\n",
        "    ensure_dir(predictions_dir)\n",
        "\n",
        "    out_csv = os.path.join(predictions_dir, f\"test_predictions_{args.exp_name}.csv\")\n",
        "    df = pd.DataFrame({\"path\": filenames, \"speaker_id\": preds})\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"[INFO] Saved predictions â†’ {out_csv}\")\n",
        "    print(\"\\nì €ì¥ëœ íŒŒì¼ì„ ëŸ°ì–´ìŠ¤ ìë£Œì‹¤ì— ì œì¶œí•´ì£¼ì„¸ìš”!\")\n",
        "\n",
        "\n",
        "def ensemble_infer(exp_names, use_tta=True, tta_crops=10):\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ëŠ” Ensemble Inference\n",
        "\n",
        "    Args:\n",
        "        exp_names: ì‹¤í—˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "        use_tta: TTA ì‚¬ìš© ì—¬ë¶€\n",
        "        tta_crops: TTA crop íšŸìˆ˜\n",
        "    \"\"\"\n",
        "    args = build_args()\n",
        "\n",
        "    logs_root = os.path.abspath(args[0].out_dir)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Label map ë¡œë“œ\n",
        "    exp_dir = os.path.join(logs_root, exp_names[0])\n",
        "    label_map = load_label_map(exp_dir)\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "    num_classes = len(inv_label_map)\n",
        "\n",
        "    print(f\"[INFO] Loading {len(exp_names)} models for ensemble...\")\n",
        "\n",
        "    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ\n",
        "    models = []\n",
        "    for exp_name in exp_names:\n",
        "        exp_dir = os.path.join(logs_root, exp_name)\n",
        "\n",
        "        model = MFAConformerSpeakerNet(\n",
        "            n_classes=num_classes,\n",
        "            input_dim=args[0].n_mels,\n",
        "            encoder_dim=args[0].encoder_dim,\n",
        "            num_blocks=args[0].num_blocks,\n",
        "            num_heads=args[0].num_heads,\n",
        "            ff_expansion=args[0].ff_expansion,\n",
        "            conv_kernel_size=args[0].conv_kernel_size,\n",
        "            dropout=args[0].dropout\n",
        "        ).to(device)\n",
        "\n",
        "        state = load_checkpoint(exp_dir, device)\n",
        "        model.load_state_dict(state[\"model_state\"], strict=True)\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "        print(f\"  âœ“ Loaded: {exp_name}\")\n",
        "\n",
        "    print(f\"[INFO] Ensemble with {len(models)} models + TTA {tta_crops}x\")\n",
        "\n",
        "    # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "    dataset = TestDataset(\n",
        "        csv_path=args[0].test_csv,\n",
        "        dataset_root=args[0].dataset_root,\n",
        "        target_sr=args[0].sr,\n",
        "        duration_sec=args[0].duration,\n",
        "        n_mels=args[0].n_mels,\n",
        "        n_fft=args[0].n_fft,\n",
        "        hop_length=args[0].hop_length,\n",
        "        apply_cmvn=True,\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        dataset, batch_size=args[0].batch_size, shuffle=False,\n",
        "        num_workers=args[0].num_workers, pin_memory=True, drop_last=False,\n",
        "    )\n",
        "\n",
        "    # ì¶”ë¡ \n",
        "    filenames = []\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"Ensemble Inference\", ncols=116)\n",
        "        for mel, filename in pbar:\n",
        "            mel = mel.to(device)\n",
        "\n",
        "            # ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘\n",
        "            all_logits = []\n",
        "\n",
        "            for model in models:\n",
        "                if use_tta:\n",
        "                    # TTA\n",
        "                    B, C, F, T = mel.shape\n",
        "                    target_len = int(args[0].duration * args[0].sr / args[0].hop_length) + 1\n",
        "\n",
        "                    logits_list = []\n",
        "                    for _ in range(tta_crops):\n",
        "                        if T > target_len:\n",
        "                            start = random.randint(0, T - target_len)\n",
        "                            mel_crop = mel[:, :, :, start:start + target_len]\n",
        "                        else:\n",
        "                            mel_crop = mel\n",
        "\n",
        "                        logits = model(mel_crop)\n",
        "                        logits_list.append(logits)\n",
        "\n",
        "                    logits = torch.stack(logits_list).mean(dim=0)\n",
        "                else:\n",
        "                    logits = model(mel)\n",
        "\n",
        "                all_logits.append(logits)\n",
        "\n",
        "            # ëª¨ë“  ëª¨ë¸ì˜ logits í‰ê· \n",
        "            ensemble_logits = torch.stack(all_logits).mean(dim=0)\n",
        "            pred_idx = ensemble_logits.argmax(dim=1).cpu().numpy().tolist()\n",
        "            pred_spk = [inv_label_map[int(i)] for i in pred_idx]\n",
        "\n",
        "            filenames.extend(filename)\n",
        "            preds.extend(pred_spk)\n",
        "\n",
        "    # CSV ì €ì¥ (logs/predictions/ í´ë”ì— - Ensembleì€ ê³ ì • ì´ë¦„)\n",
        "    predictions_dir = os.path.join(logs_root, \"predictions\")\n",
        "    ensure_dir(predictions_dir)\n",
        "\n",
        "    out_csv = os.path.join(predictions_dir, \"test_predictions.csv\")\n",
        "    df = pd.DataFrame({\"path\": filenames, \"speaker_id\": preds})\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"[INFO] Saved ensemble predictions â†’ {out_csv}\")\n",
        "    print(\"\\nì €ì¥ëœ íŒŒì¼ì„ ëŸ°ì–´ìŠ¤ ìë£Œì‹¤ì— ì œì¶œí•´ì£¼ì„¸ìš”!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzs4Qe9zzSE9"
      },
      "source": [
        "---\n",
        "\n",
        "### ëª¨ë¸ í•™ìŠµ ì‹¤í–‰í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffxLDGHQzQ9u"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/dsp_dataset/dataset\n",
        "!cp -r /content/drive/MyDrive/dsp_dataset/dataset/logs /content/logs\n",
        "\n",
        "# Model 1: Seed 42 (ê°•í•œ Regularization)\n",
        "args = build_args()\n",
        "args[0].exp_name = \"EXP_StrongReg_seed42\"\n",
        "args[0].label_smoothing = 0.15  # 0.1 â†’ 0.15\n",
        "args[0].dropout = 0.2           # 0.1 â†’ 0.2\n",
        "args[0].weight_decay = 2e-4     # 1e-4 â†’ 2e-4\n",
        "args[0].seed = 42\n",
        "args[0].epochs = 25\n",
        "\n",
        "print(\"=\" * 66)\n",
        "print(\"[Model 1/3] Training with seed=42\")\n",
        "print(\"=\" * 66)\n",
        "trainer = Trainer(args[0])\n",
        "trainer.run()\n",
        "\n",
        "# Model 2: Seed 123\n",
        "args = build_args()\n",
        "args[0].exp_name = \"EXP_StrongReg_seed123\"\n",
        "args[0].label_smoothing = 0.15\n",
        "args[0].dropout = 0.2\n",
        "args[0].weight_decay = 2e-4\n",
        "args[0].seed = 123\n",
        "args[0].epochs = 25\n",
        "\n",
        "print(\"=\" * 66)\n",
        "print(\"[Model 2/3] Training with seed=123\")\n",
        "print(\"=\" * 66)\n",
        "trainer = Trainer(args[0])\n",
        "trainer.run()\n",
        "\n",
        "# Model 3: Seed 777\n",
        "args = build_args()\n",
        "args[0].exp_name = \"EXP_StrongReg_seed777\"\n",
        "args[0].label_smoothing = 0.15\n",
        "args[0].dropout = 0.2\n",
        "args[0].weight_decay = 2e-4\n",
        "args[0].seed = 777\n",
        "args[0].epochs = 25\n",
        "\n",
        "print(\"=\" * 66)\n",
        "print(\"[Model 3/3] Training with seed=777\")\n",
        "print(\"=\" * 66)\n",
        "trainer = Trainer(args[0])\n",
        "trainer.run()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 66)\n",
        "print(\"âœ“ All 3 models trained! Ready for ensemble.\")\n",
        "print(\"=\" * 66)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx00YnfUmdef"
      },
      "source": [
        "---\n",
        "\n",
        "### ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu-egVFAmgii"
      },
      "outputs": [],
      "source": [
        "ensemble_infer(\n",
        "    exp_names=[\n",
        "        \"EXP_StrongReg_seed42\",\n",
        "        \"EXP_StrongReg_seed123\",\n",
        "        \"EXP_StrongReg_seed777\"\n",
        "    ],\n",
        "    use_tta=True,\n",
        "    tta_crops=10\n",
        ")\n",
        "# ê²°ê³¼ íŒŒì¼: logs/predictions/test_predictions.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeLjwr-te03_"
      },
      "source": [
        "---\n",
        "\n",
        "#### [ê²°ê³¼ ë¶„ì„] í…ì„œë³´ë“œ í™œìš©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AnSYXLId80n"
      },
      "outputs": [],
      "source": [
        "# Initialize directories for logging and model storage\n",
        "DIR_NAME = os.path.join(os.getcwd(), 'logs', f'{args[0].exp_name}')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {DIR_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEQAXk0fDXQ6"
      },
      "source": [
        "ë°ì´í„° í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NGplSWKKyLA"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ===== 1. CSV ë¡œë“œ =====\n",
        "train_df = pd.read_csv('./dataset/train.csv')\n",
        "valid_df = pd.read_csv('./dataset/valid.csv')\n",
        "test_df = pd.read_csv('./dataset/test.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ë°ì´í„°ì…‹ ì •ë³´\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Valid samples: {len(valid_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(f\"\\nUnique speakers in train: {train_df['speaker_id'].nunique()}\")\n",
        "print(\"\\nTrain ìƒ˜í”Œ ì˜ˆì‹œ:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# ===== 2. ë°ì´í„° í™•ì¸ =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ì˜¤ë””ì˜¤ ë°ì´í„° í™•ì¸\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample_path = train_df.iloc[0]['path']\n",
        "full_path = os.path.join('./dataset', sample_path)\n",
        "y, sr = librosa.load(full_path, sr=16000)\n",
        "mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, n_fft=400, hop_length=160)\n",
        "mel_db = librosa.power_to_db(mel)\n",
        "\n",
        "print(f\"Sample path: {sample_path}\")\n",
        "print(f\"Speaker: {train_df.iloc[0]['speaker_id']}\")\n",
        "print(f\"Audio length: {len(y)/sr:.2f}s ({len(y)} samples)\")\n",
        "print(f\"Mel shape: {mel.shape} (n_mels, time_frames)\")\n",
        "print(f\"Mel dB range: {mel_db.min():.2f} ~ {mel_db.max():.2f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.imshow(mel_db, aspect='auto', origin='lower', cmap='viridis')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(f\"Mel-spectrogram: {train_df.iloc[0]['speaker_id']}\")\n",
        "plt.xlabel('Time frames')\n",
        "plt.ylabel('Mel frequency bins')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ===== 3. ëª¨ë¸ ì¶œë ¥ í™•ì¸ =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ëª¨ë¸ ì¶œë ¥ í™•ì¸\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Trainerì—ì„œ ë§Œë“  ë°ì´í„°ë¡œë” ì‚¬ìš©\n",
        "if 'trainer' in globals():\n",
        "    loader = trainer.train_loader\n",
        "    model = trainer.model\n",
        "    device = trainer.device\n",
        "\n",
        "    for mel_batch, label_batch in loader:\n",
        "        print(f\"Batch mel shape: {mel_batch.shape}  # [batch, 1, n_mels, time]\")\n",
        "        print(f\"Batch label shape: {label_batch.shape}  # [batch]\")\n",
        "        print(f\"Mel range: {mel_batch.min():.4f} ~ {mel_batch.max():.4f}\")\n",
        "        print(f\"Labels in batch: {label_batch[:10].tolist()}\")\n",
        "\n",
        "        # ëª¨ë¸ ìˆœì „íŒŒ\n",
        "        mel_batch = mel_batch.to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(mel_batch)\n",
        "\n",
        "        print(f\"\\nModel output (logits) shape: {logits.shape}  # [batch, n_classes]\")\n",
        "        print(f\"Logits range: {logits.min():.4f} ~ {logits.max():.4f}\")\n",
        "        print(f\"Predictions: {logits.argmax(dim=1)[:10].tolist()}\")\n",
        "        print(f\"Ground truth: {label_batch[:10].tolist()}\")\n",
        "\n",
        "        # ì •í™•ë„ í™•ì¸\n",
        "        preds = logits.argmax(dim=1).cpu()\n",
        "        correct = (preds == label_batch).sum().item()\n",
        "        acc = correct / len(label_batch)\n",
        "        print(f\"\\nBatch accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "        break\n",
        "else:\n",
        "    print(\"[WARN] Trainerê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "KeLjwr-te03_"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}